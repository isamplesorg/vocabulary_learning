{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#File: dataPreprocessing.ipynb\r\n",
    "#Purpose: data preprocessing for steve mapping dataset\r\n",
    "#Author: Quan Gan\r\n",
    "import pandas as pd\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "df = pd.read_csv('../data/Raw_data/steve_mapping_1000.csv', skiprows = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "info = df[['collectionCode', 'habitat', 'higherGeography', 'locality', 'higherClassification', 'Steve label']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#recrods' labels clean\r\n",
    "info = info.replace(np.nan, \"\", regex=True)\r\n",
    "info = info[info['Steve label'] != \"\"]\r\n",
    "info = info.replace('\\n', \" \", regex=True)\r\n",
    "info = info.replace('Lake,', \"Lake\", regex=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#add prefix \"__label__\" to labels\r\n",
    "labels = info['Steve label'].str.split(r', ', expand=True)\r\n",
    "labels = labels.rename(columns= {0: \"label1\", 1: \"label2\"})\r\n",
    "labels['label1'] = \"__label__\" + labels['label1']\r\n",
    "labels = labels.fillna(\"\")\r\n",
    "labels.loc[labels['label2'] != \"\", 'label2'] = \"__label__\"+labels.loc[labels['label2'] != \"\"]['label2'] "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "result = labels.replace(\" \", \"_\", regex=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#records' text clean\r\n",
    "info['text'] = info['collectionCode'] + \" \" + info['habitat'] + \" \" + info['higherGeography'] + \" \" + info['locality'] + \" \" + info['higherClassification']\r\n",
    "info['text'] = info['text'].str.replace('[^\\w\\s]','', regex=True)\r\n",
    "info['text'] = info['text'].str.lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#sample records to trainset and testset\r\n",
    "result['label'] = result['label1'] + \" \" + result['label2']\r\n",
    "result['text'] = info['text']\r\n",
    "ds = result['label'] + \" \" + result['text']\r\n",
    "train = ds.sample(frac=0.7, random_state=99) #randomly choose 70% records as trainset\r\n",
    "test = ds.loc[~ds.index.isin(train.index)] #rest 30% records as testset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "ds.to_csv(r'../data/cleanedData.txt', header=False, index=False)\r\n",
    "train.to_csv(r'../data/steve_696.train', header=False, index=False)\r\n",
    "test.to_csv(r'../data/steve_299.valid', header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "result = result.sample(n=100, random_state=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result.to_csv('DwC_cleaned.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "small_80 = result.sample(frac=0.8, random_state=2)\r\n",
    "small_20 = result.iloc[~result.index.isin(small_80.index)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "(small_80['label']+' '+small_80['text']).to_csv('small_80.txt', header=False, index=False)\r\n",
    "(small_20['label']+' '+small_20['text']).to_csv('small_20.txt', header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "#Method: trainModel\r\n",
    "#Purpose: train fastText model\r\n",
    "#Paremater: trainSet -> the trainSet file path\r\n",
    "#           input_word_vector -> the pretrained word vector file path\r\n",
    "import fasttext\r\n",
    "def trainModel(trainSet, input_word_vector, LR, Epoch):\r\n",
    "    model = fasttext.train_supervised(input = trainSet,\r\n",
    "                                      dim = 300,\r\n",
    "                                      lr = LR,\r\n",
    "                                      epoch = Epoch,\r\n",
    "                                      loss ='ova',\r\n",
    "                                      pretrainedVectors = input_word_vector)\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "model = trainModel('small_80.txt', '../data/crawl-300d-2M-subword.vec', 0.5, 20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "pre = small_20['text'].tolist()\r\n",
    "output = pd.DataFrame(columns=['test-label', 'Probability'])\r\n",
    "output['test-label'] = model.predict(pre)[0]\r\n",
    "output['Probability'] = model.predict(pre)[1]\r\n",
    "\r\n",
    "model.test('small_20.txt',k=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(19, 0.9473684210526315, 0.9)"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "\r\n",
    "testlabel = model.test_label('small_20.txt', k=1)\r\n",
    "testlabel\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'__label__Marine_water_body_bottom': {'precision': 0.0,\n",
       "  'recall': nan,\n",
       "  'f1score': 0.0},\n",
       " '__label__Subaerial_surface_environment': {'precision': 1.0,\n",
       "  'recall': nan,\n",
       "  'f1score': 2.0},\n",
       " '__label__Lake_river_or_stream_bottom': {'precision': 1.0,\n",
       "  'recall': nan,\n",
       "  'f1score': 2.0},\n",
       " '__label__Marine_water_body': {'precision': 1.0,\n",
       "  'recall': nan,\n",
       "  'f1score': 2.0},\n",
       " '__label__Active_human_occupation_site': {'precision': nan,\n",
       "  'recall': nan,\n",
       "  'f1score': nan}}"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "pd.concat([pd.DataFrame(small_20['label']).reset_index(drop=True), output], axis=1).to_csv('result.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "c5562ae4cb73c005d4d512b63371f9759c2bba9f1255843d95070cdaa5df3d5e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}